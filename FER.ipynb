{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "FER.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G87CpbI6EGtu",
        "colab_type": "code",
        "outputId": "12cef4be-766f-4ce3-cf72-285a14ac6b9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import np_utils\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnp8bMvxW-PD",
        "colab_type": "code",
        "outputId": "f212c778-127a-4b6f-cc7c-d7e164e932db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "df = pd.read_csv('gdrive/My Drive/fer2013.csv')\n",
        "print ((df['Usage']).unique())\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Training' 'PublicTest' 'PrivateTest']\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWLjRxOKEGty",
        "colab_type": "code",
        "outputId": "16c98f5e-ec3e-419a-8f10-8dce0086ead9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "x_train, y_train, x_test, y_test = [], [], [], []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "  val = row['pixels'].split(\" \")\n",
        "  try:\n",
        "    if 'Training' in row['Usage']:\n",
        "      x_train.append(np.array(val, 'float32'))\n",
        "      y_train.append(row['emotion'])\n",
        "    elif 'PublicTest' in row['Usage']:\n",
        "      x_test.append(np.array(val, 'float32'))\n",
        "      y_test.append(row['emotion'])\n",
        "  except:\n",
        "    print (f\"error occured at index:{index} and row:{row}\")\n",
        "\n",
        "print (x_train[0:2])\n",
        "print (x_test[0:2])\n",
        "print (y_train[0:2])\n",
        "print (y_test[0:2])\n",
        "\n",
        "x_train = np.array(x_train, 'float32')\n",
        "y_train = np.array(y_train, 'float32')\n",
        "x_test = np.array(x_test, 'float32')\n",
        "y_test = np.array(y_test, 'float32')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([ 70.,  80.,  82., ..., 106., 109.,  82.], dtype=float32), array([151., 150., 147., ..., 193., 183., 184.], dtype=float32)]\n",
            "[array([254., 254., 254., ...,  42., 129., 180.], dtype=float32), array([156., 184., 198., ..., 172., 167., 161.], dtype=float32)]\n",
            "[0, 0]\n",
            "[0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcmQicoiEGt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalizing\n",
        "x_train -= np.mean(x_train, axis=0)\n",
        "x_train /= np.std(x_train, axis=0)\n",
        "\n",
        "x_test -= np.mean(x_test, axis=0)\n",
        "x_test /= np.std(x_test, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "818wnHK6EGt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_features = 64\n",
        "num_labels = 7\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "width, height = 48, 48\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], width, height, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], width, height, 1)\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train, num_classes=num_labels)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes=num_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIw-6ZDhEGt8",
        "colab_type": "code",
        "outputId": "690026fb-0cf0-4239-b928-41d7c99f24b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', input_shape=(x_train.shape[1:])))\n",
        "model.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(2*2*2*2*num_features, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(2*2*2*2*num_features, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(num_labels, activation='softmax'))\n",
        "\n",
        "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test),\n",
        "          shuffle=True)\n",
        "\n",
        "fer_json = model.to_json()\n",
        "with open(\"fer.json\", \"w\") as json_file:\n",
        "  json_file.write(fer_json)\n",
        "\n",
        "model.save_weights(\"fer.h5\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0905 15:10:59.669652 140340527249280 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0905 15:10:59.689970 140340527249280 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0905 15:10:59.693472 140340527249280 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0905 15:10:59.732160 140340527249280 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0905 15:10:59.736069 140340527249280 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0905 15:10:59.747555 140340527249280 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0905 15:10:59.941076 140340527249280 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0905 15:10:59.953445 140340527249280 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0905 15:11:00.163334 140340527249280 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 28709 samples, validate on 3589 samples\n",
            "Epoch 1/100\n",
            "28709/28709 [==============================] - 25s 871us/step - loss: 1.7430 - acc: 0.2848 - val_loss: 1.6144 - val_acc: 0.3555\n",
            "Epoch 2/100\n",
            "28709/28709 [==============================] - 23s 787us/step - loss: 1.5259 - acc: 0.3970 - val_loss: 1.4259 - val_acc: 0.4475\n",
            "Epoch 3/100\n",
            "28709/28709 [==============================] - 23s 784us/step - loss: 1.4110 - acc: 0.4472 - val_loss: 1.3115 - val_acc: 0.4935\n",
            "Epoch 4/100\n",
            "28709/28709 [==============================] - 22s 782us/step - loss: 1.3422 - acc: 0.4847 - val_loss: 1.2696 - val_acc: 0.5038\n",
            "Epoch 5/100\n",
            "28709/28709 [==============================] - 23s 790us/step - loss: 1.2934 - acc: 0.5010 - val_loss: 1.2712 - val_acc: 0.5163\n",
            "Epoch 6/100\n",
            "28709/28709 [==============================] - 23s 785us/step - loss: 1.2610 - acc: 0.5131 - val_loss: 1.2203 - val_acc: 0.5249\n",
            "Epoch 7/100\n",
            "28709/28709 [==============================] - 23s 784us/step - loss: 1.2275 - acc: 0.5298 - val_loss: 1.2010 - val_acc: 0.5383\n",
            "Epoch 8/100\n",
            "28709/28709 [==============================] - 23s 784us/step - loss: 1.1960 - acc: 0.5456 - val_loss: 1.1966 - val_acc: 0.5450\n",
            "Epoch 9/100\n",
            "28709/28709 [==============================] - 23s 787us/step - loss: 1.1756 - acc: 0.5514 - val_loss: 1.1845 - val_acc: 0.5472\n",
            "Epoch 10/100\n",
            "28709/28709 [==============================] - 23s 784us/step - loss: 1.1504 - acc: 0.5587 - val_loss: 1.1657 - val_acc: 0.5517\n",
            "Epoch 11/100\n",
            "28709/28709 [==============================] - 22s 779us/step - loss: 1.1331 - acc: 0.5671 - val_loss: 1.1668 - val_acc: 0.5506\n",
            "Epoch 12/100\n",
            "28709/28709 [==============================] - 23s 787us/step - loss: 1.1146 - acc: 0.5748 - val_loss: 1.1714 - val_acc: 0.5475\n",
            "Epoch 13/100\n",
            "28709/28709 [==============================] - 23s 789us/step - loss: 1.0895 - acc: 0.5841 - val_loss: 1.1694 - val_acc: 0.5553\n",
            "Epoch 14/100\n",
            "28709/28709 [==============================] - 23s 785us/step - loss: 1.0738 - acc: 0.5907 - val_loss: 1.1472 - val_acc: 0.5600\n",
            "Epoch 15/100\n",
            "28709/28709 [==============================] - 23s 785us/step - loss: 1.0610 - acc: 0.5936 - val_loss: 1.1483 - val_acc: 0.5645\n",
            "Epoch 16/100\n",
            "28709/28709 [==============================] - 23s 784us/step - loss: 1.0419 - acc: 0.6026 - val_loss: 1.1491 - val_acc: 0.5670\n",
            "Epoch 17/100\n",
            "28709/28709 [==============================] - 23s 788us/step - loss: 1.0275 - acc: 0.6090 - val_loss: 1.1686 - val_acc: 0.5656\n",
            "Epoch 18/100\n",
            "28709/28709 [==============================] - 23s 787us/step - loss: 1.0098 - acc: 0.6140 - val_loss: 1.1415 - val_acc: 0.5706\n",
            "Epoch 19/100\n",
            "28709/28709 [==============================] - 22s 782us/step - loss: 0.9890 - acc: 0.6235 - val_loss: 1.1368 - val_acc: 0.5807\n",
            "Epoch 20/100\n",
            "28709/28709 [==============================] - 22s 782us/step - loss: 0.9711 - acc: 0.6277 - val_loss: 1.1517 - val_acc: 0.5723\n",
            "Epoch 21/100\n",
            "28709/28709 [==============================] - 23s 790us/step - loss: 0.9657 - acc: 0.6312 - val_loss: 1.1473 - val_acc: 0.5759\n",
            "Epoch 22/100\n",
            "28709/28709 [==============================] - 23s 787us/step - loss: 0.9436 - acc: 0.6417 - val_loss: 1.1653 - val_acc: 0.5762\n",
            "Epoch 23/100\n",
            "28709/28709 [==============================] - 22s 781us/step - loss: 0.9303 - acc: 0.6448 - val_loss: 1.1661 - val_acc: 0.5684\n",
            "Epoch 24/100\n",
            "28709/28709 [==============================] - 22s 781us/step - loss: 0.9262 - acc: 0.6491 - val_loss: 1.1403 - val_acc: 0.5834\n",
            "Epoch 25/100\n",
            "28709/28709 [==============================] - 22s 783us/step - loss: 0.9081 - acc: 0.6528 - val_loss: 1.1670 - val_acc: 0.5756\n",
            "Epoch 26/100\n",
            "28709/28709 [==============================] - 22s 782us/step - loss: 0.8939 - acc: 0.6615 - val_loss: 1.2213 - val_acc: 0.5768\n",
            "Epoch 27/100\n",
            "28709/28709 [==============================] - 23s 789us/step - loss: 0.8832 - acc: 0.6640 - val_loss: 1.1654 - val_acc: 0.5868\n",
            "Epoch 28/100\n",
            "28709/28709 [==============================] - 23s 786us/step - loss: 0.8628 - acc: 0.6707 - val_loss: 1.1871 - val_acc: 0.5765\n",
            "Epoch 29/100\n",
            "28709/28709 [==============================] - 23s 789us/step - loss: 0.8522 - acc: 0.6774 - val_loss: 1.1938 - val_acc: 0.5681\n",
            "Epoch 30/100\n",
            "28709/28709 [==============================] - 22s 781us/step - loss: 0.8479 - acc: 0.6778 - val_loss: 1.1955 - val_acc: 0.5779\n",
            "Epoch 31/100\n",
            "28709/28709 [==============================] - 22s 782us/step - loss: 0.8344 - acc: 0.6850 - val_loss: 1.1966 - val_acc: 0.5701\n",
            "Epoch 32/100\n",
            "28709/28709 [==============================] - 23s 786us/step - loss: 0.8138 - acc: 0.6906 - val_loss: 1.2451 - val_acc: 0.5770\n",
            "Epoch 33/100\n",
            "28709/28709 [==============================] - 22s 783us/step - loss: 0.7987 - acc: 0.6986 - val_loss: 1.2001 - val_acc: 0.5818\n",
            "Epoch 34/100\n",
            "28709/28709 [==============================] - 23s 784us/step - loss: 0.7945 - acc: 0.7000 - val_loss: 1.2034 - val_acc: 0.5734\n",
            "Epoch 35/100\n",
            "28709/28709 [==============================] - 23s 784us/step - loss: 0.7756 - acc: 0.7075 - val_loss: 1.2500 - val_acc: 0.5795\n",
            "Epoch 36/100\n",
            "28709/28709 [==============================] - 22s 781us/step - loss: 0.7734 - acc: 0.7124 - val_loss: 1.2560 - val_acc: 0.5704\n",
            "Epoch 37/100\n",
            "28709/28709 [==============================] - 23s 787us/step - loss: 0.7601 - acc: 0.7127 - val_loss: 1.2431 - val_acc: 0.5801\n",
            "Epoch 38/100\n",
            "28709/28709 [==============================] - 23s 786us/step - loss: 0.7461 - acc: 0.7189 - val_loss: 1.2178 - val_acc: 0.5790\n",
            "Epoch 39/100\n",
            "28709/28709 [==============================] - 22s 780us/step - loss: 0.7394 - acc: 0.7210 - val_loss: 1.2655 - val_acc: 0.5776\n",
            "Epoch 40/100\n",
            "28709/28709 [==============================] - 23s 786us/step - loss: 0.7339 - acc: 0.7259 - val_loss: 1.3035 - val_acc: 0.5723\n",
            "Epoch 41/100\n",
            "28709/28709 [==============================] - 22s 780us/step - loss: 0.7302 - acc: 0.7269 - val_loss: 1.2686 - val_acc: 0.5704\n",
            "Epoch 42/100\n",
            "28709/28709 [==============================] - 23s 788us/step - loss: 0.7039 - acc: 0.7368 - val_loss: 1.2810 - val_acc: 0.5807\n",
            "Epoch 43/100\n",
            "28709/28709 [==============================] - 23s 788us/step - loss: 0.7131 - acc: 0.7348 - val_loss: 1.2690 - val_acc: 0.5801\n",
            "Epoch 44/100\n",
            "28709/28709 [==============================] - 23s 791us/step - loss: 0.6976 - acc: 0.7430 - val_loss: 1.3216 - val_acc: 0.5773\n",
            "Epoch 45/100\n",
            "28709/28709 [==============================] - 22s 780us/step - loss: 0.6821 - acc: 0.7459 - val_loss: 1.3508 - val_acc: 0.5656\n",
            "Epoch 46/100\n",
            "28709/28709 [==============================] - 22s 778us/step - loss: 0.6803 - acc: 0.7449 - val_loss: 1.2931 - val_acc: 0.5793\n",
            "Epoch 47/100\n",
            "28709/28709 [==============================] - 22s 782us/step - loss: 0.6693 - acc: 0.7514 - val_loss: 1.3169 - val_acc: 0.5701\n",
            "Epoch 48/100\n",
            "28709/28709 [==============================] - 22s 783us/step - loss: 0.6602 - acc: 0.7536 - val_loss: 1.3116 - val_acc: 0.5754\n",
            "Epoch 49/100\n",
            "28709/28709 [==============================] - 22s 780us/step - loss: 0.6569 - acc: 0.7570 - val_loss: 1.3324 - val_acc: 0.5823\n",
            "Epoch 50/100\n",
            "28709/28709 [==============================] - 22s 782us/step - loss: 0.6434 - acc: 0.7594 - val_loss: 1.3968 - val_acc: 0.5756\n",
            "Epoch 51/100\n",
            "28709/28709 [==============================] - 22s 780us/step - loss: 0.6408 - acc: 0.7630 - val_loss: 1.3474 - val_acc: 0.5695\n",
            "Epoch 52/100\n",
            "28709/28709 [==============================] - 22s 781us/step - loss: 0.6318 - acc: 0.7683 - val_loss: 1.3338 - val_acc: 0.5723\n",
            "Epoch 53/100\n",
            "28709/28709 [==============================] - 22s 780us/step - loss: 0.6373 - acc: 0.7655 - val_loss: 1.3736 - val_acc: 0.5829\n",
            "Epoch 54/100\n",
            "28709/28709 [==============================] - 22s 777us/step - loss: 0.6219 - acc: 0.7717 - val_loss: 1.3473 - val_acc: 0.5729\n",
            "Epoch 55/100\n",
            "28709/28709 [==============================] - 22s 783us/step - loss: 0.6069 - acc: 0.7762 - val_loss: 1.3460 - val_acc: 0.5784\n",
            "Epoch 56/100\n",
            "28709/28709 [==============================] - 23s 787us/step - loss: 0.6166 - acc: 0.7731 - val_loss: 1.3172 - val_acc: 0.5773\n",
            "Epoch 57/100\n",
            "28709/28709 [==============================] - 22s 776us/step - loss: 0.5949 - acc: 0.7790 - val_loss: 1.3717 - val_acc: 0.5756\n",
            "Epoch 58/100\n",
            "28709/28709 [==============================] - 22s 782us/step - loss: 0.5935 - acc: 0.7817 - val_loss: 1.3850 - val_acc: 0.5782\n",
            "Epoch 59/100\n",
            "28709/28709 [==============================] - 22s 777us/step - loss: 0.6026 - acc: 0.7792 - val_loss: 1.4094 - val_acc: 0.5762\n",
            "Epoch 60/100\n",
            "28709/28709 [==============================] - 22s 778us/step - loss: 0.5829 - acc: 0.7874 - val_loss: 1.3861 - val_acc: 0.5809\n",
            "Epoch 61/100\n",
            "28709/28709 [==============================] - 22s 779us/step - loss: 0.5727 - acc: 0.7900 - val_loss: 1.4123 - val_acc: 0.5768\n",
            "Epoch 62/100\n",
            "28709/28709 [==============================] - 22s 781us/step - loss: 0.5817 - acc: 0.7873 - val_loss: 1.4194 - val_acc: 0.5807\n",
            "Epoch 63/100\n",
            "28709/28709 [==============================] - 22s 779us/step - loss: 0.5687 - acc: 0.7907 - val_loss: 1.4470 - val_acc: 0.5784\n",
            "Epoch 64/100\n",
            "28709/28709 [==============================] - 22s 783us/step - loss: 0.5579 - acc: 0.7944 - val_loss: 1.4267 - val_acc: 0.5840\n",
            "Epoch 65/100\n",
            "28709/28709 [==============================] - 22s 783us/step - loss: 0.5562 - acc: 0.7991 - val_loss: 1.4796 - val_acc: 0.5751\n",
            "Epoch 66/100\n",
            "28709/28709 [==============================] - 22s 780us/step - loss: 0.5389 - acc: 0.8027 - val_loss: 1.4568 - val_acc: 0.5846\n",
            "Epoch 67/100\n",
            "28709/28709 [==============================] - 23s 785us/step - loss: 0.5527 - acc: 0.7996 - val_loss: 1.4583 - val_acc: 0.5762\n",
            "Epoch 68/100\n",
            "28709/28709 [==============================] - 22s 782us/step - loss: 0.5395 - acc: 0.8068 - val_loss: 1.4825 - val_acc: 0.5807\n",
            "Epoch 69/100\n",
            "28709/28709 [==============================] - 23s 785us/step - loss: 0.5432 - acc: 0.8028 - val_loss: 1.4786 - val_acc: 0.5737\n",
            "Epoch 70/100\n",
            "28709/28709 [==============================] - 22s 782us/step - loss: 0.5282 - acc: 0.8097 - val_loss: 1.4754 - val_acc: 0.5717\n",
            "Epoch 71/100\n",
            "28709/28709 [==============================] - 22s 778us/step - loss: 0.5312 - acc: 0.8066 - val_loss: 1.4938 - val_acc: 0.5832\n",
            "Epoch 72/100\n",
            "28709/28709 [==============================] - 22s 783us/step - loss: 0.5269 - acc: 0.8096 - val_loss: 1.5148 - val_acc: 0.5818\n",
            "Epoch 73/100\n",
            "28709/28709 [==============================] - 22s 782us/step - loss: 0.5151 - acc: 0.8118 - val_loss: 1.4449 - val_acc: 0.5715\n",
            "Epoch 74/100\n",
            "28709/28709 [==============================] - 22s 778us/step - loss: 0.5266 - acc: 0.8081 - val_loss: 1.4651 - val_acc: 0.5737\n",
            "Epoch 75/100\n",
            "28709/28709 [==============================] - 22s 777us/step - loss: 0.5127 - acc: 0.8131 - val_loss: 1.5305 - val_acc: 0.5642\n",
            "Epoch 76/100\n",
            "28709/28709 [==============================] - 22s 777us/step - loss: 0.5118 - acc: 0.8133 - val_loss: 1.4934 - val_acc: 0.5890\n",
            "Epoch 77/100\n",
            "28709/28709 [==============================] - 23s 784us/step - loss: 0.5020 - acc: 0.8194 - val_loss: 1.4614 - val_acc: 0.5751\n",
            "Epoch 78/100\n",
            "28709/28709 [==============================] - 23s 785us/step - loss: 0.4988 - acc: 0.8201 - val_loss: 1.4766 - val_acc: 0.5734\n",
            "Epoch 79/100\n",
            "28709/28709 [==============================] - 22s 781us/step - loss: 0.4954 - acc: 0.8230 - val_loss: 1.4856 - val_acc: 0.5743\n",
            "Epoch 80/100\n",
            "28709/28709 [==============================] - 22s 781us/step - loss: 0.4896 - acc: 0.8231 - val_loss: 1.5540 - val_acc: 0.5701\n",
            "Epoch 81/100\n",
            "28709/28709 [==============================] - 22s 783us/step - loss: 0.5025 - acc: 0.8208 - val_loss: 1.4844 - val_acc: 0.5768\n",
            "Epoch 82/100\n",
            "28709/28709 [==============================] - 23s 785us/step - loss: 0.4868 - acc: 0.8260 - val_loss: 1.5330 - val_acc: 0.5787\n",
            "Epoch 83/100\n",
            "28709/28709 [==============================] - 23s 787us/step - loss: 0.4931 - acc: 0.8223 - val_loss: 1.5345 - val_acc: 0.5734\n",
            "Epoch 84/100\n",
            "28709/28709 [==============================] - 22s 783us/step - loss: 0.4769 - acc: 0.8287 - val_loss: 1.4890 - val_acc: 0.5854\n",
            "Epoch 85/100\n",
            "28709/28709 [==============================] - 22s 782us/step - loss: 0.4863 - acc: 0.8233 - val_loss: 1.5481 - val_acc: 0.5776\n",
            "Epoch 86/100\n",
            "28709/28709 [==============================] - 22s 783us/step - loss: 0.4698 - acc: 0.8318 - val_loss: 1.5398 - val_acc: 0.5687\n",
            "Epoch 87/100\n",
            "28709/28709 [==============================] - 23s 784us/step - loss: 0.4553 - acc: 0.8357 - val_loss: 1.5388 - val_acc: 0.5726\n",
            "Epoch 88/100\n",
            "28709/28709 [==============================] - 23s 786us/step - loss: 0.4682 - acc: 0.8346 - val_loss: 1.5511 - val_acc: 0.5748\n",
            "Epoch 89/100\n",
            "28709/28709 [==============================] - 22s 777us/step - loss: 0.4544 - acc: 0.8368 - val_loss: 1.6414 - val_acc: 0.5667\n",
            "Epoch 90/100\n",
            "28709/28709 [==============================] - 22s 782us/step - loss: 0.4566 - acc: 0.8357 - val_loss: 1.6470 - val_acc: 0.5737\n",
            "Epoch 91/100\n",
            "28709/28709 [==============================] - 23s 787us/step - loss: 0.4625 - acc: 0.8340 - val_loss: 1.6097 - val_acc: 0.5690\n",
            "Epoch 92/100\n",
            "28709/28709 [==============================] - 22s 783us/step - loss: 0.4555 - acc: 0.8400 - val_loss: 1.6349 - val_acc: 0.5720\n",
            "Epoch 93/100\n",
            "28709/28709 [==============================] - 23s 784us/step - loss: 0.4729 - acc: 0.8336 - val_loss: 1.5780 - val_acc: 0.5665\n",
            "Epoch 94/100\n",
            "28709/28709 [==============================] - 22s 778us/step - loss: 0.4555 - acc: 0.8389 - val_loss: 1.6179 - val_acc: 0.5826\n",
            "Epoch 95/100\n",
            "28709/28709 [==============================] - 23s 787us/step - loss: 0.4474 - acc: 0.8410 - val_loss: 1.6209 - val_acc: 0.5779\n",
            "Epoch 96/100\n",
            "28709/28709 [==============================] - 22s 783us/step - loss: 0.4442 - acc: 0.8420 - val_loss: 1.5853 - val_acc: 0.5709\n",
            "Epoch 97/100\n",
            "28709/28709 [==============================] - 23s 785us/step - loss: 0.4437 - acc: 0.8407 - val_loss: 1.6216 - val_acc: 0.5865\n",
            "Epoch 98/100\n",
            "28709/28709 [==============================] - 22s 778us/step - loss: 0.4440 - acc: 0.8428 - val_loss: 1.6517 - val_acc: 0.5737\n",
            "Epoch 99/100\n",
            "28709/28709 [==============================] - 22s 784us/step - loss: 0.4326 - acc: 0.8477 - val_loss: 1.6606 - val_acc: 0.5720\n",
            "Epoch 100/100\n",
            "28709/28709 [==============================] - 22s 781us/step - loss: 0.4408 - acc: 0.8418 - val_loss: 1.6431 - val_acc: 0.5734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX-Uu3-DHlO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLyHzY53HoeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}